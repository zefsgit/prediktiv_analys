{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a SMS Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset information\n",
    "\n",
    "Data set containing 5,572 Text Messages and their corresponding label (target): \n",
    "- **ham**: 4,828 observations\n",
    "- **spam**: 747 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms = pd.read_table('./data/sms.tsv', header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n \nOk lar... Joking wif u oni...\n \nFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n \nU dun say so early hor... U c already then say...\n \nNah I don't think he goes to usf, he lives around here though\n \n"
     ]
    }
   ],
   "source": [
    "for m in sms['message'][:5]:\n",
    "    print(m, end='\\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert label to a numerical variable: 1 (positive class) will be \"spam\"\n",
    "sms['target'] = (sms['label'] == 'spam').astype(int)\n",
    "sms.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vector-representation (Bag of words)\n",
    "\n",
    "In general (not only with bag of words) there are 2 steps to produce a vector representation for each document:\n",
    "\n",
    "1. Learn the vocubulary in the corpus: this done using the fit method.\n",
    "2. Use that vocabulary to produce the vector representation for each document: this is done using the transform method.\n",
    "\n",
    "Scikit learn provides the <code>fit_transform</code> method to perfom the 2 steps at the same time."
   ]
  },
  {
   "source": [
    "## Example of Bag of words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of a corpus. Corpus the set of all documets. A document is an oberservation (a message)\n",
    "# Every word in the corpus is a feature\n",
    "# The bag of words count how many times a word appear in a document\n",
    "corpus = [\n",
    "          'This is the first document',\n",
    "          'This is the second second document',\n",
    "          'And the third one. Yes, yes, yes this',\n",
    "          'Is this the first document?'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# step1: learn the 'vocabulary' of the training data (occurs in-place)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['and',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this',\n",
       " 'yes']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 2: Vectorize the documents (dtm: document-token matrix)\n",
    "X_dtm = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       and  document  first  is  one  second  the  third  this  yes\n",
       "doc_1    0         1      1   1    0       0    1      0     1    0\n",
       "doc_2    0         1      0   1    0       2    1      0     1    0\n",
       "doc_3    1         0      0   0    1       0    1      1     1    3\n",
       "doc_4    0         1      1   1    0       0    1      0     1    0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>and</th>\n      <th>document</th>\n      <th>first</th>\n      <th>is</th>\n      <th>one</th>\n      <th>second</th>\n      <th>the</th>\n      <th>third</th>\n      <th>this</th>\n      <th>yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>doc_1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>doc_2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>doc_3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>doc_4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Bag of words representation\n",
    "pd.DataFrame(data=X_dtm.toarray(), columns=vectorizer.get_feature_names(), index=['doc_'+str(i+1) for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words for SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_name = 'target'\n",
    "X = sms['message']\n",
    "y = sms[target_name]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "# Producing the document-token matrix (in one step)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "# transform testing data (using fitted vocabulary) into a document-token matrix\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CMatrix(CM, labels=['ham','spam']):\n",
    "    df = pd.DataFrame(data=CM, index=labels, columns=labels)\n",
    "    df.index.name='TRUE'\n",
    "    df.columns.name='PREDICTION'\n",
    "    df.loc['Total'] = df.sum()\n",
    "    df['Total'] = df.sum(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 99.29%\nAccuracy: 99.1%\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PREDICTION  ham  spam  Total\n",
       "TRUE                        \n",
       "ham         965     1    966\n",
       "spam          9   140    149\n",
       "Total       974   141   1115"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>PREDICTION</th>\n      <th>ham</th>\n      <th>spam</th>\n      <th>Total</th>\n    </tr>\n    <tr>\n      <th>TRUE</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ham</th>\n      <td>965</td>\n      <td>1</td>\n      <td>966</td>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <td>9</td>\n      <td>140</td>\n      <td>149</td>\n    </tr>\n    <tr>\n      <th>Total</th>\n      <td>974</td>\n      <td>141</td>\n      <td>1115</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# 1. Import the estimator object (model)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 2. Create an instance of the estimator\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# 3. Use the trainning data to train the estimator\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "y_pred_test = nb.predict(X_test_dtm)\n",
    "precision = precision_score(y_pred=y_pred_test, y_true=y_test)\n",
    "accuracy = accuracy_score(y_pred=y_pred_test, y_true=y_test)\n",
    "#Confusion matrix\n",
    "CM = confusion_matrix(y_pred=y_pred_test, y_true=y_test)\n",
    "# F-strings is a formatting making able to give varaibles as input in strings by insering them in {}. Ex:\n",
    "#name = 'Eva'\n",
    "#year = 29\n",
    "#f\"Hi my name is {name} and I'm {year} years old.\"\n",
    "print(f\"Precision: {round(100*precision, 2)}%\")\n",
    "print(f\"Accuracy: {round(100*accuracy, 2)}%\")\n",
    "\n",
    "CMatrix(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the porportion of cases where our cases are correct when we are predicting spam. \n",
    "\n",
    "** Lets predict the class for the following sms: **\n",
    "1. \"Today is your lucky day! claim $100 of free gas now! just text back saying YES.\"\n",
    "2. \"I have been calling you all day, r u comming back before dinner?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spam_filter(text_message):\n",
    "    \"\"\"Accepts a string containing a text message and classifies it as spam or ham\"\"\"\n",
    "    prediction = nb.predict(vect.transform([text_message]))[0]\n",
    "    print(text_message)\n",
    "    if prediction:\n",
    "        return \"SPAM\"\n",
    "    else:\n",
    "        return \"HAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms1 = \"Today is your lucky day! claim $100 of free gas now! just text back saying YES.\"\n",
    "sms2 = \"I have been calling you all day, r u comming back before dinner?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I have been calling you all day, r u comming back before dinner?\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'HAM'"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "spam_filter(sms2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! you have built your first text classifier!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}